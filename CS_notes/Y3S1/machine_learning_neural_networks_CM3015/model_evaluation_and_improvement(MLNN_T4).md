---
tags: [bias-variance-curve, regularisation, gradiant-descent, overfitting, underfitting, loss-function, n-fold-cross-validation, n-fold-nested-cross-validation, bayesian-modelling, bayes-theorem, naive-bayes-classifier]
aliases: [MLNN T4, Machine Learning and Neural Networks Topic 4]
---

# Reading resources for this topic

1. [Chapter 4 on overfitting, cross-validation and handling test sets](https://ebookcentral.proquest.com/lib/londonww/detail.action?docID=6642860)
2. [Section 2.7 model selection and generalisation](https://ebookcentral.proquest.com/lib/londonww/detail.action?docID=3339851)
3. [1.2.1 Naive bayes](https://ebookcentral.proquest.com/lib/londonww/detail.action?docID=6642860)
4. [Probabilistic modelling 3.1 - 3.2](https://ebookcentral.proquest.com/lib/londonww/detail.action?docID=3339851)

# Undesirable fitting

![[high_bias_underfitting.png]]

![[high_variance_overfit.png]]

# Ideal fitting

## William of Ockham's quote

> plurality should not be posited without necessity

In other words, keep it simple!

## Bias-variance curve

- allows you to plot along the x-axis some measure of model complexity
- y axis is the amount of error

![[between_underfit_overfit.png]]

## Ensure model is generalisable

1. Reduce the number of features
	- Manually select which features to keep
	- Use model-selection algorithm (e.g cross-validation)
2. Regularisation
	- Keep all features, but reduce the values of parameters θ
	- Works well when we have a lot of features

# Regularisation

> add penalty to the loss function based on complexity

Regularisation works by:
- adding a parameter to the loss function that penalises complexity in the model
	- the regularisation term is added to artificially increase the loss if the parameters in the model have become too complex (variable and large). This forces the learning algorithm (e.g. gradient descent) to optimise for a simplere model
- encouraging the training algorithm (e.g. gradient descent) to optimise for a simpler, more generalisable model
	- done by penalising any complexity in the model

![[add_penalty_on_complexity.png]]

## Loss function

$$ J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta (x^i)-y^i)^2 $$
To regularise this loss function,

$$  J(\theta) = \frac{1}{2m}\sum_{i=1}^m(h_\theta (x^i)-y^i)^2 + \lambda \sum_{j=1}^n \theta_j^2 $$

Regularisation hyperparameter, lambda

If lambda is too big?
	- algorithm underfits (theta very small)

If lambda is too small?
	- algorithm may overfit (theta can be very large)

# Gradient Descent and Regularisation

Randomly initialise θ, then loop:

1. Calculate loss J(θ)
2. Update:
$$ \theta_0^{new}=\theta_0 - \frac{\alpha}{m}\sum_{i=1}^m (h_\theta(x^i)-y^i) \text{ Do not regularise } \theta_0 $$
$$ \theta_0^{new}=\theta_0 - \frac{\alpha}{m}\sum_{i=1}^m (h_\theta(x^i)-y^i)x_j^i-\lambda\theta_j \text{ for j = 1, ..., n} $$
3. End loop when converged, i.e J(θ) approximates to J(θ^new)

# Cross-validation

To avoid overfitting, it is important that the data we use to train the algorithm is not the same as the one we use to test it.

## Train-test separation

Usually, we do not have enough data and hence we use N-fold Cross Validation to maximise the use of our data. 
- This is done by splitting our data into n sections.
- We then take one portion and set it aside to be tested while the remaining is used for training
- Process is repeated on the remaining n-1 portions

![[n_fold_cross_validation.png]]

$$ e_i = \text{error in fold i} $$
$$ e_{total} = \frac{1}{n}\sum_{i=1}^n e_i $$

![[issues_with_n_fold_cross_validation.png]]

![[solution_for_issues_on_n_fold_cross_validation.png]]

# N-fold Nested Cross Validation

![[n_fold_nested_cross_validation.png]]

![[n_fold_nested_cross_validation_loop.png]]

# The Curse of Dimensionality

![[dimensionality.png]]

- adding a new dimension may help in better separate the classes

However, adding a new dimension also comes with new problems:

![[problems_of_dimensionality.png]]

- data points start becoming sparse and widely spread

## Less is more

![[less_is_more.png]]

- with more dimensions, more features, the space get exponentially bigger, especially so if dataset is fixed and so the spread becomes larger

# Bayesian modelling

Probabilistic modelling is a way of doing machine learning that allows us to deal with uncertainty. With probabilistic modelling, we can output probabilities or degrees of uncertainty in a particular outcome.

prior probability:
P(good graduate job) = 0.50

posterior probability:
P(good graduate job | attend networking event) = 0.80

## Bayes theorem

$$ Posterior = \frac{\text{Likelihood * Prior}}{\text{Marginal Probability}} $$

Posterior - probability of event given evidence
Likelihood - likelihood evidence was generated by event
Prior - what we believed about event beforehand
Marginal Probability - ensures posterior sums to 1 over all possible events

## Probability Rules

![[probability_rule.png]]

Inverse
$$ P(\bar A) = 1 - P(A) $$

Conditional
$$ P(B | A) $$

Product Rule
$$ P(B, A) = P(B | A)P(A) = P(A | B)P(B) $$

Sum Rule
$$ P(B) = P(B, A) + P(B, \bar A) = P(B | A)P(A) + P(B | \bar A)P(\bar A) $$

Given X are inputs and Y are outputs, 

Product Rule: 
$$ P(Y, X) = P(Y|X)*P(X) $$

Sum Rule:
$$ P(X) = \sum_Y P(X, Y) $$

Bayes Theorem:
$$ P(Y, X) = P(X, Y) $$
$$ P(Y|X)*P(X) = P(X|Y)*P(Y) $$
$$ P(Y|X) = \frac{P(X|Y)*P(Y)}{P(X)} $$
$$ \text{normalising constant with } P(X) = \sum_Y P(X|Y)P(Y) $$

## Doctor Bayes' test

How likely is that you have the disease given that you test positive?

![[doctor_bayes_test_example.png]]

## Allergy test

![[allergy_test.png]]

# Generative Bayesian classifier

- a kind of generative model
- we build a model that we train up to recognise positive instances of whatever we want it to identify by providing it with lots of evidence

![[generative_bayesian_classifier_with_marginal_probability.png]]

- However, by removing the marginal probability, we are able to create other models 

![[generative_bayesian_classifier_without_marginal_probability.png]]

# Naive Bayes classifier

![[naive_bayes.png]]

![[bayes_with_multiple_features.png]]

- expanding the model with multiple features
- need to take care as all of these observations are not independent from one another and thus, we have to look at various combinations between the different features
- meaning that if we expand using chain rule or probability calculation it can become complicated really quickly

However, if we make an assumption that all of these features are independent of one another, we can reduce the product rule to just be individual probabilities multiplied together of each feature given that we know the class. This means that the likelihood of each class can be evaluated independently of all the other features.

- this assumption is naive because it is often not the case
- in the real world, features are very often interlinked

## Naive Bayes in practice

- marginalizing constant is removed, thus there is an approximation of the posterior
- formula shows we are taking prior P(Y) and multiplying it by all of the independent likelihoods for each feature we have
- an issue arise from this whereby since each independent likelihoods are very small values and we are multiplying it together, the number will become smaller and smaller every time
Therefore,
- a solution is to take the logarithm of the posterior, prior and the likelihood values

![[naive_bayes_in_practice.png]]